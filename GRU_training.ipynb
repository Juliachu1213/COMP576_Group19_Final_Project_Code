{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# metrics & plotting\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    roc_auc_score,\n",
        "    roc_curve,\n",
        ")\n",
        "\n",
        "# to run on colab\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Choose mode\n",
        "# -----------------------------\n",
        "FEATURE_MODE = \"sentiment\"   # 'sentiment'(method 1) / 'market' / 'both'(method 2)\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Read sentiment + DJIA, and merge by date\n",
        "# -----------------------------\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "    # sentiment data\n",
        "    sent_path = \"/content/drive/MyDrive/Colab Notebooks/COMP576/news_label_daily_sentiment_scores.csv\"\n",
        "    df_sent = pd.read_csv(sent_path)\n",
        "\n",
        "    # DJIA data\n",
        "    djia_path = \"/content/drive/MyDrive/Colab Notebooks/COMP576/upload_DJIA_table.csv\"\n",
        "    df_djia = pd.read_csv(djia_path)\n",
        "\n",
        "    df_sent[\"Date\"] = pd.to_datetime(df_sent[\"Date\"])\n",
        "    df_djia[\"Date\"] = pd.to_datetime(df_djia[\"Date\"])\n",
        "    df = pd.merge(df_sent, df_djia, on=\"Date\", how=\"inner\").sort_values(\"Date\").reset_index(drop=True)  # merge only when dates match\n",
        "    print(df[[\"Date\", \"sentiment_score\", \"Close\"]].head())\n",
        "    print(df[[\"Date\", \"sentiment_score\", \"Close\"]].tail())\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Drive mount failed or file not found. Please check paths.\")\n",
        "    raise e\n",
        "\n",
        "print(\"Number of data：\", len(df))\n",
        "print(\"Range of date：\", df[\"Date\"].min(), \"~\", df[\"Date\"].max())\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Model configuration\n",
        "# -----------------------------\n",
        "batch_size = 64\n",
        "test_batch_size = 100\n",
        "epochs = 500\n",
        "lr = 0.0003\n",
        "try_cuda = True\n",
        "sinseed = 1000\n",
        "HIDDEN_SIZE = 64\n",
        "window_size = 5           # sliding window size\n",
        "\n",
        "if torch.cuda.is_available() and try_cuda:\n",
        "    cuda = True\n",
        "    torch.cuda.manual_seed(sinseed)\n",
        "else:\n",
        "    cuda = False\n",
        "    torch.manual_seed(sinseed)\n",
        "\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Choose input data\n",
        "# -----------------------------\n",
        "possible_market_cols = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"]\n",
        "market_cols = [c for c in possible_market_cols if c in df.columns]\n",
        "\n",
        "if FEATURE_MODE == \"sentiment\":\n",
        "    feature_cols = [\"sentiment_score\"]\n",
        "elif FEATURE_MODE == \"market\":\n",
        "    if len(market_cols) == 0:\n",
        "        raise ValueError(\"Can't find [Open/High/Low/Close/Volume...], check upload_DJIA_table.csv\")\n",
        "    feature_cols = market_cols\n",
        "elif FEATURE_MODE == \"both\":\n",
        "    if len(market_cols) == 0:\n",
        "        raise ValueError(\"Can't find [Open/High/Low/Close/Volume...], check upload_DJIA_table.csv\")\n",
        "    feature_cols = [\"sentiment_score\"] + market_cols\n",
        "else:\n",
        "    raise ValueError(\"FEATURE_MODE has to be one of 'sentiment' / 'market' / 'both'\")\n",
        "\n",
        "print(\"Feature columns:\", feature_cols)\n",
        "\n",
        "raw_features = df[feature_cols].values.astype(np.float32)  # (N, feature_dim)\n",
        "Labels = df[\"Label\"].values.astype(np.int64)\n",
        "Dates  = df[\"Date\"].values\n",
        "\n",
        "# -----------------------------\n",
        "# 5. Separate train / test and use train-only fit scaler\n",
        "# -----------------------------\n",
        "split_date = pd.Timestamp(\"2014-12-31\")\n",
        "\n",
        "train_mask = (df[\"Date\"] <= split_date).values\n",
        "if not train_mask.any():\n",
        "    raise ValueError(\"No date in data <= 2014-12-31\")\n",
        "\n",
        "split_idx = np.where(train_mask)[0].max()\n",
        "print(\"Last Day index for training =\", split_idx, \"Date =\", df.loc[split_idx, \"Date\"])\n",
        "\n",
        "# fit scaler with train data to avoid data leakage\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(raw_features[:split_idx + 1])\n",
        "Features = scaler.transform(raw_features)\n",
        "\n",
        "feature_dim = Features.shape[1]\n",
        "INPUT_SIZE = feature_dim\n",
        "print(\"feature_dim =\", feature_dim)\n",
        "\n",
        "# -----------------------------\n",
        "# 6. Sliding window\n",
        "# -----------------------------\n",
        "past_train, labels_train = [], []\n",
        "past_test, labels_test   = [], []\n",
        "\n",
        "N = len(df)\n",
        "\n",
        "# train: window ends at split_idx\n",
        "for t in range(window_size, split_idx + 1):\n",
        "    past_train.append(Features[t - window_size:t, :])\n",
        "    labels_train.append(Labels[t])\n",
        "\n",
        "# test: label index > split_idx\n",
        "for t in range(split_idx + 1, N):\n",
        "    if t - window_size < 0:\n",
        "        continue\n",
        "    past_test.append(Features[t - window_size:t, :])\n",
        "    labels_test.append(Labels[t])\n",
        "\n",
        "past_train = np.array(past_train, dtype=np.float32)\n",
        "labels_train = np.array(labels_train, dtype=np.int64)\n",
        "past_test = np.array(past_test, dtype=np.float32)\n",
        "labels_test = np.array(labels_test, dtype=np.int64)\n",
        "\n",
        "print(\"Train X shape:\", past_train.shape)\n",
        "print(\"Train y shape:\", labels_train.shape)\n",
        "print(\"Test  X shape:\", past_test.shape)\n",
        "print(\"Test  y shape:\", labels_test.shape)\n",
        "\n",
        "train_dataset = TensorDataset(torch.from_numpy(past_train), torch.from_numpy(labels_train))\n",
        "test_dataset  = TensorDataset(torch.from_numpy(past_test),  torch.from_numpy(labels_test))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# -----------------------------\n",
        "# 7. 建立模型\n",
        "# -----------------------------\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.rnn = nn.GRU(\n",
        "            input_size=INPUT_SIZE,\n",
        "            hidden_size=HIDDEN_SIZE,\n",
        "            num_layers=2,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.out = nn.Linear(HIDDEN_SIZE, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        r_out, h_n = self.rnn(x)\n",
        "        last_hidden = r_out[:, -1, :]\n",
        "        dropped_out = self.dropout(last_hidden)\n",
        "        logits = self.out(dropped_out)\n",
        "        return logits\n",
        "\n",
        "model = Net().to(device)\n",
        "\n",
        "# -----------------------------\n",
        "# 8. class weights, loss, optimizer\n",
        "# -----------------------------\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels_train), y=labels_train)\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "print(f\"Class Weights: {class_weights}\")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "\n",
        "# -----------------------------\n",
        "# 9. train & test\n",
        "# -----------------------------\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for X_batch, y_batch in loader:\n",
        "        X_batch = X_batch.to(device).float()\n",
        "        y_batch = y_batch.to(device).long()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X_batch)\n",
        "        loss = criterion(logits, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * X_batch.size(0)\n",
        "    return running_loss / len(loader.dataset)\n",
        "\n",
        "def test(model, loader, device, criterion=None, return_details=False):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in loader:\n",
        "            X_batch = X_batch.to(device).float()\n",
        "            y_batch = y_batch.to(device).long()\n",
        "            logits = model(X_batch)\n",
        "            probs = torch.softmax(logits, dim=1)[:, 1]  # probability of class 1\n",
        "\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.append(preds.cpu().numpy())\n",
        "            all_labels.append(y_batch.cpu().numpy())\n",
        "            all_probs.append(probs.cpu().numpy())\n",
        "\n",
        "            if criterion is not None:\n",
        "                loss = criterion(logits, y_batch)\n",
        "                total_loss += loss.item() * X_batch.size(0)\n",
        "\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "    all_probs = np.concatenate(all_probs)\n",
        "\n",
        "    prop_ones = np.mean(all_preds)\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1  = f1_score(all_labels, all_preds)\n",
        "    avg_loss = total_loss / len(loader.dataset) if criterion is not None else 0.0\n",
        "\n",
        "    try:\n",
        "        auc = roc_auc_score(all_labels, all_probs)\n",
        "    except Exception:\n",
        "        auc = float(\"nan\")\n",
        "\n",
        "    if return_details:\n",
        "        return acc, f1, prop_ones, avg_loss, auc, all_preds, all_labels, all_probs\n",
        "    else:\n",
        "        return acc, f1, prop_ones, avg_loss, auc\n",
        "\n",
        "# -----------------------------\n",
        "# 10. training loop\n",
        "# ### Select epoch with highest AUC as the benchmark(best epoch)\n",
        "# -----------------------------\n",
        "print(f\"{'Epoch':^5} | {'Loss':^8} | {'TrnAcc':^8} | {'Trn1s%':^8} | {'TstAcc':^8} | {'TstF1':^8} | {'TstAUC':^8} | {'TstLoss':^8} | {'Tst1s%':^8}\")\n",
        "print(\"-\"*110)\n",
        "\n",
        "# for plotting line charts\n",
        "test_loss_hist = []\n",
        "test_acc_hist  = []\n",
        "test_f1_hist   = []\n",
        "test_auc_hist  = []\n",
        "\n",
        "best_test_auc = -np.inf\n",
        "best_epoch = None\n",
        "best_state_dict = None      # store best epoch model's parameters\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    train_acc, train_f1, train_prop, train_eval_loss, train_auc = test(model, train_loader, device, criterion)\n",
        "    test_acc,  test_f1,  test_prop,  test_loss,  test_auc  = test(model, test_loader,  device, criterion)\n",
        "\n",
        "    test_loss_hist.append(test_loss)\n",
        "    test_acc_hist.append(test_acc)\n",
        "    test_f1_hist.append(test_f1)\n",
        "    test_auc_hist.append(test_auc)\n",
        "\n",
        "    # select best epoch\n",
        "    if not np.isnan(test_auc) and test_auc > best_test_auc:\n",
        "        best_test_auc = test_auc\n",
        "        best_epoch = epoch\n",
        "        best_state_dict = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "\n",
        "    print(\n",
        "        f\"{epoch:02d}    | \"\n",
        "        f\"{train_loss:.4f}   | \"\n",
        "        f\"{train_acc:.4f}   | \"\n",
        "        f\"{train_prop:.2%}   | \"\n",
        "        f\"{test_acc:.4f}   | \"\n",
        "        f\"{test_f1:.4f}   | \"\n",
        "        f\"{(test_auc if not np.isnan(test_auc) else 0):.4f}   | \"\n",
        "        f\"{test_loss:.4f}   | \"\n",
        "        f\"{test_prop:.2%}\"\n",
        "    )\n",
        "\n",
        "print(f\"\\n>>> Best epoch by Test AUC: {best_epoch}  (AUC = {best_test_auc:.4f})\")\n",
        "\n",
        "# change model weight to the best epoch version\n",
        "if best_state_dict is not None:\n",
        "    model.load_state_dict(best_state_dict)\n",
        "    model.to(device)\n",
        "\n",
        "# -----------------------------\n",
        "# 11. Draw line charts: Test Loss / Test Acc / Test F1 / Test AUC\n",
        "# -----------------------------\n",
        "epochs_range = range(1, len(test_loss_hist) + 1)\n",
        "\n",
        "plt.figure(figsize=(18, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(epochs_range, test_loss_hist, label=\"Test Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Test Loss per Epoch\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(epochs_range, test_acc_hist, label=\"Test Accuracy\", color=\"green\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Test Accuracy per Epoch\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(epochs_range, test_f1_hist, label=\"Test F1 Score\", color=\"red\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"F1 Score\")\n",
        "plt.title(\"Test F1 Score per Epoch\")\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(epochs_range, test_auc_hist, label=\"Test ROC-AUC\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"AUC\")\n",
        "plt.title(\"Test ROC-AUC per Epoch\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# 12. Last test(with best epoch's model): Directional Accuracy / ROC-AUC / Confusion Matrix / Baseline\n",
        "# -----------------------------\n",
        "final_acc, final_f1, final_prop, final_loss, final_auc, final_preds, final_labels, final_probs = \\\n",
        "    test(model, test_loader, device, criterion, return_details=True)\n",
        "\n",
        "print(\"\\n===== Final Evaluation on Test Set (Best Epoch) =====\")\n",
        "print(f\"Best Epoch: {best_epoch}\")\n",
        "print(f\"Directional Accuracy (Accuracy): {final_acc:.4f}\")\n",
        "print(f\"F1 Score: {final_f1:.4f}\")\n",
        "print(f\"ROC-AUC: {final_auc:.4f}\")\n",
        "print(f\"Test Loss: {final_loss:.4f}\")\n",
        "print(f\"Predicted 1's proportion: {final_prop:.2%}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(final_labels, final_preds)\n",
        "print(\"\\n===== Confusion Matrix (rows=true, cols=pred) =====\")\n",
        "print(cm)\n",
        "\n",
        "# -----------------------------\n",
        "# 13. Draw ROC Curve(with best epoch's model)\n",
        "# -----------------------------\n",
        "fpr, tpr, thresholds = roc_curve(final_labels, final_probs)\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {final_auc:.3f})\")\n",
        "plt.plot([0, 1], [0, 1], linestyle=\"--\", label=\"Random guess\")\n",
        "plt.xlabel(\"False Positive Rate (FPR)\")\n",
        "plt.ylabel(\"True Positive Rate (TPR)\")\n",
        "plt.title(\"ROC Curve on Test Set (Best Epoch)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W8gMgPd3ci4X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}